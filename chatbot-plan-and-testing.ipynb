{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategic Leadership Copilot\n",
    "\n",
    "## Plan\n",
    "\n",
    "Build a copilot within my data and ai leadership course for mcmaster that allows students to query course information. This includes text content, case studies etc.\n",
    "\n",
    "Requirements\n",
    "- Chatbot functionality\n",
    "- RAG on course content\n",
    "- Students can send their documents to chat with\n",
    "- For prompt engineering, create a system that allows me to assess their prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft Architecture\n",
    "\n",
    "- Frontend: Static Web App - Azure\n",
    "- Backend: App Service - Azure: \n",
    "  - Makes calls to GPT \n",
    "  - Persists state\n",
    "  - \n",
    "  - \n",
    "- Document Storage & Upload: Blob for document upload and RAG on course content.\n",
    "- Chat memory: [Need to find cheapest option] Cosmos DB used in other architectures, here I \n",
    "- Key Vault: Authentication & secrets [in production, in dev we just doing whatever]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature List - Frontend\n",
    "- Chat interface\n",
    "- Upload button\n",
    "- Markdown rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, I encountered an error.\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "from flask import Flask, request, jsonify\n",
    "import json\n",
    "\n",
    "user_message = \"Hey there, how are you?\"\n",
    "try:\n",
    "\n",
    "    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n",
    "    client = AzureOpenAI(\n",
    "        # if no key specified gets the API Key from environment variable AZURE_OPENAI_API_KEY\n",
    "        api_key = \"a7c02d6cccc44acaa1207229de52ef26\",\n",
    "        api_version=\"2024-08-01-preview\",\n",
    "        # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n",
    "        azure_endpoint=\"https://open-ai-service-class.openai.azure.com/\",\n",
    "    )\n",
    "\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",  # e.g. gpt-35-instant\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message,\n",
    "        },\n",
    "    ],\n",
    "    )\n",
    "    print(completion.choices[0].text.strip())\n",
    "except Exception as e:\n",
    "    print(\"Sorry, I encountered an error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#rest-api-versioning\n",
    "client = AzureOpenAI(\n",
    "    # if no key specified gets the API Key from environment variable AZURE_OPENAI_API_KEY\n",
    "    api_key = \"a7c02d6cccc44acaa1207229de52ef26\",\n",
    "    api_version=\"2024-08-01-preview\",\n",
    "    # https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource\n",
    "    azure_endpoint=\"https://open-ai-service-class.openai.azure.com/\",\n",
    ")\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "model=\"gpt-4o-mini\",  # e.g. gpt-35-instant\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message,\n",
    "    },\n",
    "],\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from chatbot: Personality theory explores the consistent patterns in thoughts, feelings, and behaviors that characterize individuals. Key frameworks include:\n",
      "\n",
      "1. **Trait Theory**: Focuses on specific traits (e.g., the Big Five: openness, conscientiousness, extraversion, agreeableness, neuroticism) that describe personalities.\n",
      "  \n",
      "2. **Psychoanalytic Theory**: Emphasizes unconscious motives and childhood experiences (e.g., Freud's id, ego, superego).\n",
      "  \n",
      "3. **\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "start_url = \"https://daiol-chatbot-c7c6bhf0cghgdtdj.canadacentral-01.azurewebsites.net/api/start\"\n",
    "\n",
    "response = requests.get(start_url)\n",
    "if response.status_code == 200:\n",
    "    session_token = response.json().get(\"token\")\n",
    "else:\n",
    "    print(f\"Failed to start a session. Status code: {response.status_code}\")\n",
    "    exit()\n",
    "\n",
    "chat_url = \"https://daiol-chatbot-c7c6bhf0cghgdtdj.canadacentral-01.azurewebsites.net/api/chat\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": session_token\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"message\": \"Explain to me personality theory in a few words\"\n",
    "}\n",
    "\n",
    "\n",
    "# Make a POST request to the API\n",
    "try:\n",
    "    response = requests.post(chat_url, headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(\"Response from chatbot:\", response.json()[\"response\"])\n",
    "    else:\n",
    "        print(f\"Failed to get a valid response. Status code: {response.status_code}\")\n",
    "        print(\"Response content:\", response.text)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's absolutely feasible to have users upload documents and then perform RAG on those documents, but there are practical considerations that often guide whether this is the best approach. Here are some reasons why RAG might not be used dynamically on user-uploaded documents, along with when it can be feasible:\n",
    "\n",
    "### Practical Considerations for RAG on User-Uploaded Documents\n",
    "\n",
    "1. **Document Indexing and Processing Time**:\n",
    "   - RAG works best when documents are pre-processed, indexed, and stored in a format that allows fast retrieval. If a user uploads a new document every time, the system would need to index it on-the-fly, which could introduce delays.\n",
    "   - The indexing process involves generating embeddings (vectors) for each chunk, which can be computationally expensive, especially for long documents. Doing this dynamically for every upload can affect the response time.\n",
    "\n",
    "2. **Scalability and Performance**:\n",
    "   - RAG systems are generally optimized for larger, static datasets where multiple queries can be handled efficiently after the data is indexed once. For dynamic, ad-hoc uploads, the system would have to repeat the indexing process for each document, which might not scale well.\n",
    "   - Performance can degrade if the system needs to repeatedly perform retrieval operations on freshly uploaded, non-indexed data.\n",
    "\n",
    "3. **Security and Data Privacy**:\n",
    "   - Storing documents for indexing and retrieval might raise concerns about data privacy and security. Ensuring the safety of sensitive data can be more challenging when handling dynamic uploads.\n",
    "   - When documents are static and stored securely, it’s easier to enforce robust privacy policies.\n",
    "\n",
    "4. **Token Limitations**:\n",
    "   - If RAG is not used and a large document is uploaded, the entire document might exceed the token limits of language models like GPT. Chunking and retrieval help mitigate this by only sending relevant parts to the model.\n",
    "   - RAG helps with large static data, but dynamic processing would still need a way to manage how much text can be passed to the model at once.\n",
    "\n",
    "### When Dynamic RAG Makes Sense\n",
    "\n",
    "1. **Short Documents or Limited Use Cases**:\n",
    "   - If the uploaded documents are relatively short (or can be chunked into manageable pieces quickly), then on-the-fly RAG could work well. \n",
    "   - For example, if users are uploading customer support tickets or small reports, you could dynamically index these documents for RAG without too much delay.\n",
    "\n",
    "2. **Building a Temporary Index**:\n",
    "   - You can design a system to temporarily index the uploaded document, perform RAG to answer user queries, and then discard the index after a session ends. This would allow dynamic interaction without long-term storage.\n",
    "   - This might be useful for applications where users need quick, document-specific answers but don’t require persistent storage.\n",
    "\n",
    "3. **Hybrid Approach**:\n",
    "   - For frequently uploaded, similar types of documents, you can use a hybrid approach where some static elements are pre-indexed, and dynamic parts are added temporarily when uploaded.\n",
    "   - For example, a legal document search tool might have a database of standard contracts (pre-indexed) and allow users to add a new contract temporarily for more specific searches.\n",
    "\n",
    "### Why RAG Isn't Used Dynamically in My Current Setup\n",
    "\n",
    "In my case, the process doesn’t involve dynamically storing and indexing user-uploaded documents for several reasons:\n",
    "- **Real-Time Response Needs**: The system aims to provide quick, immediate answers without the overhead of dynamically creating indices, which can introduce delays.\n",
    "- **Privacy Concerns**: Storing user documents temporarily or permanently for indexing could raise privacy issues. Analyzing them directly without indexing avoids these complications.\n",
    "- **Simplicity and Efficiency**: The current design focuses on efficiently understanding and responding to the provided content without building and querying a temporary index each time.\n",
    "\n",
    "### Feasibility for Your Application\n",
    "\n",
    "If you’re considering using RAG dynamically:\n",
    "- **Optimize for Speed**: Use fast indexing methods and ensure the retriever is efficient. You may need a powerful backend that can handle embeddings and retrieval quickly.\n",
    "- **Limit Document Size**: Consider placing limits on document size or chunking them so the system can work within reasonable time constraints.\n",
    "- **Ensure Data Security**: Build strong encryption and privacy protocols, especially if you’re working with sensitive user data.\n",
    "\n",
    "In summary, dynamic RAG is possible and can be effective for some use cases, but it requires careful engineering to handle indexing, retrieval speed, and data privacy. For large, frequently queried datasets, static RAG is generally more efficient and practical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Prompt:\n",
    "\n",
    "Okay so if I'm understanding this correctly, let me explain to you what I'm thinking and you offer any suggestions if I'm incorect: I'm building a course in Data, AI and Organizational Leadership and want to add a strategic leadership chatbot onto it. Would it make sense to perform RAG on the course content (textbook, data etc) to allow the course informaiton to be quickly and accurately retrieved, but allow students to upload docs and simply send to you through the api? Or is there a better way?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your approach makes a lot of sense! Here's a breakdown of how it could work and why it’s effective, along with some suggestions:\n",
    "\n",
    "### 1. **Pre-Index Course Content Using RAG**\n",
    "   - **Index Core Course Materials**: You can use RAG to pre-process and index all your core course content (textbooks, lecture notes, data sets, etc.). This will allow the chatbot to quickly retrieve relevant information when students ask questions. By doing this, you're essentially creating a knowledge base that the chatbot can query to provide accurate and context-specific answers.\n",
    "   - **Benefits**: \n",
    "     - **Fast and Efficient**: Because the content is already indexed, retrieval will be much faster compared to processing new data dynamically each time.\n",
    "     - **Scalable**: As your course grows or gets updated, you can simply re-index the new content.\n",
    "\n",
    "### 2. **Allow Students to Upload Documents Dynamically**\n",
    "   - **Handle Student Uploads Separately**: When students upload their documents (e.g., assignments, personal notes, external reading materials), you can treat this as a dynamic, on-the-fly process. Instead of indexing these documents permanently, you can send the content to the OpenAI API to analyze or answer specific queries. This way, each upload is treated independently.\n",
    "   - **Benefits**:\n",
    "     - **Privacy**: Since these uploads are processed dynamically, you don’t have to store or index them, which minimizes data security concerns.\n",
    "     - **Flexibility**: Students can get help analyzing their unique documents without affecting the core RAG system.\n",
    "\n",
    "### 3. **How It Would Work Together**\n",
    "   - **Core Course Queries**: When students ask questions related to the course, the chatbot uses RAG to pull information from pre-indexed materials. For example, if a student asks, \"What is the role of AI in strategic leadership?\", the system retrieves relevant content from the course and generates an answer.\n",
    "   - **Dynamic Document Queries**: If a student uploads a case study or their notes and asks a question, you would extract the text from the uploaded file and send it directly to the API for processing. This could include summarizing the document, answering questions, or analyzing specific parts.\n",
    "\n",
    "### 4. **Suggestions for Implementation**\n",
    "   - **Hybrid Query System**: Build a system where the chatbot first checks if a question can be answered using the core course content (via RAG). If not, it can then offer to let the student upload a document for a more personalized answer.\n",
    "   - **Chunking and Context Management**: For larger student documents, you might want to use chunking to ensure that each piece of text sent to the API stays within token limits. Maintain context between chunks if the query requires it.\n",
    "   - **User Experience**: Make the process smooth for students by clearly guiding them on how to ask questions or upload documents. For example, the chatbot could say, \"I can answer questions based on our course materials. If you’d like to analyze something external, please upload your document.\"\n",
    "\n",
    "### **Example Workflow**\n",
    "1. **Student Query 1**: \"Can you explain the concept of machine learning from the course?\"\n",
    "   - The chatbot retrieves pre-indexed material using RAG and provides a concise explanation.\n",
    "   \n",
    "2. **Student Query 2**: \"Here’s a report I’m working on. Can you help summarize the main points?\" (uploads a file)\n",
    "   - The chatbot extracts the content, sends it to the OpenAI API for summarization, and provides a response.\n",
    "\n",
    "### **Why This Works Well**\n",
    "- **Speed and Efficiency**: Pre-indexing allows for fast retrieval, while dynamic processing gives flexibility for unique requests.\n",
    "- **Scalability**: You can handle many students at once because most common queries are handled by the pre-indexed data.\n",
    "- **Better User Experience**: Students get quick answers to common course-related questions and can still receive personalized help when needed.\n",
    "\n",
    "By combining the strengths of RAG for core content and dynamic processing for uploads, you can create a robust and responsive strategic leadership chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B - Chat memory example\n",
    "\n",
    "Below is the code for how we get our chatbot to maintain a history of conversations - just some demo code - I have since updated this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "client = OpenAI(api_key=\"<OPENAI_API_TOKEN>\")\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful math tutor.\"}]\n",
    "user_msgs = [\"Explain what pi is.\", \"Summarize this in two bullet points.\"]\n",
    "\n",
    "for q in user_msgs:\n",
    "    print(\"User: \", q)\n",
    "    \n",
    "    # Create a dictionary for the user message from q and append to messages\n",
    "    user_dict = {\"role\": \"user\", \"content\": q}\n",
    "    messages.append(user_dict)\n",
    "    \n",
    "    # Create the API request\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages = messages,\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    # Convert the assistant's message to a dict and append to messages\n",
    "    assistant_dict = {\"role\": \"assistant\", \"content\": response.choices[0].message.content}\n",
    "    messages.append(assistant_dict)\n",
    "    print(\"Assistant: \", response.choices[0].message.content, \"\\n\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
